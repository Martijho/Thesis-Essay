\chapter{Experiment 3: Relearning a task}
\label{exp3}
The motivation behind reusing modules is as mentioned in the introduction, to increase the number of tasks a PathNet can learn before being saturated by decreasing the amount of new capacity a path needs for a new task. A optimal search algorithm for new paths should therefore be able to reuse as much knowledge as possible without limiting the performance. If we assume a ideal search algorithm would find optimal modules for each task, it should be able to find and reuse all learned knowledge if a task is learned twice. 

We can put this to a test by learning some task twice for all seven algorithms in chapter \ref{exp2}. As the amount of module reuse in these experiments were small, we lower the proverbial bar here by reducing the module search space.

The question we want to answer is this: \textit{"Given optimal conditions for module reuse\footnote{By optimal conditions we mean learning the same task twice in a small PathNet structure.}, which selection pressure scheme is best suited for knowledge reuse?"}

\section{Description}
The experimental structure here is simple. The same experimental parameters as the experiment in chapter \ref{exp2} are used again, but here the the seven algorithms will here be applied to a binary task problem where both tasks are the same: a cSVHN classification problem with all 10 classes. After searching for a optimal path for both tasks, the reuse is noted. 

By performing this experimental trial for each algorithm multiple times, we can compare the observations of reuse with the probability of seeing these results in random path selection, and then with each other. 

\section{Hypothesis}
Based on the reuse in the previous selection pressure experiments, we do not expect any algorithm to yield perfect results\footnote{Perfect results here would constitute finding all modules used in the previous path}, but have a statistically significant difference from random path selection. Given the algorithms different selection pressures, they are expected to give somewhat different reuse results and I would expect the higher exploration algorithms to yield more reuse, as the amount of parameter optimization needed to give high fitness is relatively small. The advantage the high tournament size algorithms (algorithm 1b and to some degree 2a, 2b, 3a and 3b) have by evaluating more paths should be reduced, as these algorithms have a higher selection pressure and converge quicker. 

\section{Implementation}
The experiment structure here is a binary task problem, where we want to find two paths for a full cSVHN classification problem twice, meaning both task 1 and task 2 is cSVHN classification with all classes. The two searches should have a non-zero probability of reusing no modules, so the PathNet dimensions are set to 3 layers of 6 modules each with a maximum of 3 active modules pr. paths layer. We can describe the total number of possible paths as 
\begin{equation*}
    \prod_{i=0}^{\omega-1}(M-i)^{L}
\end{equation*}
where \(\omega\) is the maximum number of active modules in each layer for each path, \(M\) is the number of modules in each PathNet layer and  \(L\) is the number of layers in the PathNet. This means the number of possible paths in a 20-by-3 PathNet with \(\omega=3\) is  
\begin{equation*}
    \prod_{i=0}^{3-1}(20-i)^{3}\approx 3.2\times10^{11}
\end{equation*}
and in this experiment
\begin{equation*}
    \prod_{i=0}^{3-1}(6-i)^{3}\approx 1.7\times10^{6}
\end{equation*}
Reducing the PathNet size from 20 modules in each layer to 6 have reduced the search space with 5 orders of magnitude, and it should therefore be significantly easier to find reusable modules here than in the previous experiment.

We can approximate the probability of finding any number of reuse by randomly selecting two paths within the limitations of the PathNet dimensions and path-size with a Monte-Carlo approach. Using equation \ref{eq:montecarloP} where we want the probabilities to be minimum 10 times as large as the standard deviation (\(R=0.1\)) and \(n=10^{9}\) Monte-Carlo trials, we calculate the probability accuracy to be 
\begin{equation*}
    \frac{1}{nR^{2}+1}=\frac{1}{10^{7}+1}\approx10^{-7}
\end{equation*}
Meaning we can tell the probability of reuse to the seventh decimal place from the Monte-Carlo results. The results of the performed Monte-Carlo estimation can be found in table \ref{tab:montecarlo}.

\input{Chapters/4.Experiments/exp3/tables/motecarlo.tex}

For comparison to the experimental results, we will calculate the expected reuse for the number of experimental trial run, and compare the mean reuse value to the mean reuse of each algorithm. The means will be compared through ANOVA tests with a significance level of 0.05  
\section{Results}
\section{Discussion}
\section{Conclusion}
