\begin{abstract}
How is it a human brain is seemingly capable of learning an endless amount of tasks? Could we reach the same capability in our artificial minds? As machine learning becomes increasingly better at problem solving, the logical step is to push the progress towards building systems capable of solving multiple tasks to a satisfactory level. 

In this thesis, the area of efficiently reusing knowledge from one task to another is explored through experiments on a Super Neural Network structure, PathNet. Three experiments are performed, the first of which tests the properties of parameters optimized in a classical machine learning context by comparing to parameters achieved by performing a modular search, in which the parameters are optimized as part of multiple models. 

The second and third experiments investigates the genetic algorithm commonly used for optimizing the selection of parameters. By tweaking the algorithms ability to explore possibilities, seven different search schemes are tested on a set of six classification problems, all within the image domains of handwritten digits and images of house numbers. 

\end{abstract}

\iffalse
    What is all this about? 
    Why should I read this thesis? 
    Is it any good? 
    What's new? 
\fi
