\chapter{Introduction}

Biology and nature have always been imitated in art and the sciences, but over the years the imitations are growing increasingly better. Artificial Intelligence and its sub-field of Machine Learning is one of these areas. While Machine Learning are a valuable tool in itself, the ultimate goal of AI is to build what is referred to as \textit{Artificial General Intelligence}: A system capable of not only human-level performance in one field but able to generalize across domains and teach itself multiple new skills. 

As we again draw upon nature for inspiration and insights into how such a system might be constructed, the brains capability of learning new skills based on old ones is one ability is seem necessary our future  artificial minds have. Who of us can recall something they learned that were learned without any previous knowledge to base it on? Before children learn to multiply, they learn how to add. The time before that is spent counting fingers. Even riding a bike requires a level of balance and understanding of locomotion developed much earlier.

\section{Motivation}
Our brains are capable of learning something based on previous knowledge without distorting that reused knowledge. This distortion, as discussed in section \ref{background:catastrophicforgetting}, is called catastrophic forgetting and is a problem addressed and to some degree solved by technologies such as EWC\cite{ewc}, PNNs\cite{progressiveneuralnetworks} and the PathNet structure\cite{pathnet}. These introduce the ability to retain parameters optimized for task while a new task is introduced. 

However, these three solutions have limitations. EWC optimize the same set of parameters to multiple tasks, which puts restrictions on which area in parameter-space the model can occupy. PNN and PathNet both are theoretically unlimited in the amount of tasks they can retain. In practice, however, restrictions in hardware and feasible training time governs the amount of tasks and parameters in these structures. 

Optimizing the reuse of parameters between tasks can reduce the overall need for new capacity for each task. Efficiently applying old knowledge to new problems could therefore be a step towards creating vast multi-task learning systems. 

\section{Goal}
To investigate the possibilities of effective knowledge reuse, this thesis explores the PathNet structure. A Super Neural Network forgoes the monolithic architecture of traditional machine learning models in favour of a modular approach to parameter optimization. The three experiments will address how the modular training affects the transferability of knowledge between tasks and in what area of a exploration/exploitation landscape should a search algorithm focus.

\section{Outline}
\subsubsection{Chapter 1. Introduction}
Providing an introduction to the thesis as well as the motivation behind it.
\subsubsection{Chapter 2. Theoretical Background}
Lays the theoretical foundation drawn upon in this thesis. The previous work done on the PathNet structure is mentioned her.
\subsubsection{Chapter 3. Implementation}
Information about code implementation and what data is used.
\subsubsection{Chapter 4. Experiment 1: Selection versus search}
The first experiment where module permutation is done in an empty PathNet structure. The experiment explores the possibility of not performing a full path search for the first path, but rather selecting the first modules arbitrarily.
\subsubsection{Chapter 5. Experiment 2: Selection pressure}
Exploring search algorithms used for optimal module selection. Multiple searches are performed where the selection pressure of the search algorithms are different. 
\subsubsection{Chapter 6. Experiment 3: Relearning a task}
Searching for a optimal path through a PathNet to solve a task which the structure already knows. Testing if the different algorithms have search properties which provides better or worse knowledge reuse.
\subsubsection{Chapter 7. Discussion and conclusion}
Discussing and concluding the over-all experimental results reached in this thesis. Possible future work is also mentioned. 



\iffalse
\section{Problem/hypothesis}

where do i start?
Question DeepMind left unanswered is how different GAs influence task learning and module reuse. 
Exploration vs exploitation\ref{background:GA}

why this? broad answers first, specify later. 
We know PN works. would it work better for different algorithms?
logical next step from original paper "unit of evolution"




* What do modular PN training do with the knowledge? 
- More/less accuracy?
- More/less transferability? 
Test by learning in end-to-end first then PN search. 
Difference in performance or reuse?

* Can we make reuse easier by shifting focus of search algorithm?
- PN original: Naive search. Higher exploitation improve on module selection?

\section{How to answer?}
- Set up simple multitask scenarios and try. 
* 2 tasks where first are end to end vs PN
* List algorithms with different selection pressure and try on multiple tasks. 


    What is the use of a Nifty Gadget? 
    What is the problem? 
    How can it be solved? 
    What are the previous approaches? 
    What is your approach? 
    Why do it this way? 
    What are your results? 
    Why is this better? 
    Is this a new approach? 
    Why haven't anyone done it before? 
    or
    Why do you reiterate previous work? 
    What is your contribution to the field of Nifty Gadgets? 
    
    \section{What should this chapter contain?}
    Presentation of the problem or phenomenon to be addressed, the situation where the problem or phenomenon occurs, and references to earlier relevant research. 
    \subsection{Common errors}
    Problem is not properly specified or formulated; insufficient references to earlier work.  
    
    \section{Purpose}
    What can be gained by more knowledge about the problem or phenomenon. 
    \subsection{Common errors}
    The purpose is not mentioned, not connected to earlier research, or not in line with what the actual contents of the thesis.  
    
    \section{Problem/Hypothesis} 
    Questions that need to be answered to reach 
    the goal and/or hypothesis formulated be means of 
    underlying theories. 
    \subsection{Common errors}
    Missing problem description; deficiencies in the connections between questions; badly formulated 
    hypothesis.  
    
    \section{Method} 
    Choice of an adequate method with respect to the 
    purpose and problem/hypothesis. 
    
    \subsection{Common errors}
    An inappropriate method is used, for example due to lack of knowledge about different methods; 
    erroneous use of chosen method.  
\fi